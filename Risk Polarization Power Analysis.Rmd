---
title: "Risk Polarization Power Analysis"
author: "Andy Cao"
date: "`r Sys.Date()`"
output: 
   html_document:
      #css: styles.css
      toc: true
      number_sections: false
      toc_float: true
      collapsed: true
      smooth_controll: false
      fig.width: 26
      fig.height: 26
      code_download: true
---
```{r Setup, include=FALSE}
library(tidyverse) #data wrangling and other tools for R
library(knitr) # report generation in R
library(psych) #calculate skew and kurtosis for BC
library(agrmt) #for agreement and polarization calculation
library(visdat) #visualize dataframes in plots
library(RColorBrewer) #easy to use color palettes
library(rmarkdown) #for the paged_table function
library(doParallel) #parallel computation using multiple cores
library(foreach) # for each function, so the simulation does not take ages

#for those of you trying to change the thresholds, please do it here:
BC_threshold <- 5/9
polarization_threshold <- .5
group_divergence_threshold <- .5




colors <- brewer.pal(4, "Dark2")

n_scale_on_likert <- c(1:11)
min_likert <- min(n_scale_on_likert)
max_likert <- max(n_scale_on_likert)

sample_sequence <- c(seq(10,200,by =10))
sampled <- data.frame(N_part = sample_sequence)

replications_per_setting <- 200

n_population <- 10^4
prop_minority <- .05

#set seed so every random thing here is reproducible
set.seed(42)

#set echo = FALSE (e.g. dont show code in output) for all chunks, except when explicitly telling otherwise
knitr::opts_chunk$set(
   echo = FALSE,
   warning = TRUE,
   message = TRUE
   )
```
# How Many Ratings Are Needed?

## Introduction

This work builds on my [first (and failed) attempt](Risk-Polarization-Simulation.html) to estimate how many ratings one needs to sample for obtaining a dependable measure of a certain population. Therefore, readers are advised to catch up on my previous work to be up to speed on this smaller project.  

As the unconventional approach did not yield the desired results, we will revert back to the more "old school" methodologies of an a priori power analysis. That is, estimating the minimum sample size needed to uncover a set effect size (or larger), given a significance level and type II (false negatives) error.  

Though in light of such an endeavor, we have two problems:  

- Setting a (numerical) effect size depends on the polarization measure itself, and in how we define polarization in the first place.  
- We do not have an actual significance test for polarization, hence we can not define when a distribution is polarized or not.  

To solve these problems, the following proposals were taken for this work:  

- 4 preset distributions were chosen, each with differing grade of polarization (and hence different effect size), and will serve as proxies for our assumed population distributions for 4 different risks.  
- For the replacement of a polarization test, we calculate the measures of bimodality coefficient (BC), polarization and group divergence. These measures were chosen from the prior simulation work, as they seemed like good measures of polarization. We will then proceed in categorizing a distribution as "polarized" when values reach a preset threshold, and "not polarized" if the value falls below the threshold.  


# Method

## Population Distributions
While most of the previous distributions were used again (with the same color as last time) some adjustments were made. Notably, we've transitioned to a 11 -point likert scale, which our group has chosen to use for the upcoming study. Here are the distributions with some comments:  

- [**None**]{style="color: `r colors[1]`;"} (a normal Distribution, but this time with a shifted mean instead of the midpoint), representing a risk where a consensus is established.    
- [**Small polarization**]{style="color: `r colors[2]`;"} (the skewed beta distribution from prior work was replaced with this one, representing a smaller polarization in the population)  
- [**Rare**]{style="color: `r colors[3]`;"} (a majority rating on one extreme, while a minority group with base rate of ``r prop_minority *100``% on the other extreme)  
- [**Strong polarization**]{style="color: `r colors[4]`;"} (still the symmetrical version, but this time in discrete shape)  

It is important to note that while the normal and strong polarization distribution represent no and strong effects, the small and rare distributions should both illustrate small effects in the population, and putting one over the other really depends on how to interpret/ operationalize polarization itself (e.g. asymmetry, distance, agreement etc...).  

```{r Generation of Population Distribution}

normal <- round(rnorm(n_population, mean = 8,sd = 1.2))
normal <- pmin(pmax(normal, min_likert), max_likert)

strong_pol <- round(rbeta(n_population, shape1 = .1, shape2 = .1)*10) +1
strong_pol <- pmin(pmax(strong_pol, min_likert), max_likert)

rare <- c(round(rnorm(n_population *(1- prop_minority), mean = 2,sd = 1.3)), 
           round(rnorm(n_population *prop_minority, mean = 11,sd = .5)))
rare <- pmin(pmax(rare, min_likert), max_likert)

small_pol <- c(round(rnorm(n_population *.65, mean = 5,sd = .9)), 
           round(rnorm(n_population * .35, mean = 10,sd = .5)))
small_pol <- pmin(pmax(small_pol, min_likert), max_likert)


Pop_df <- data.frame(`Normal Distribution`= normal,
                     `Small Polarization` = small_pol,
                     `Rare Polarization` = rare,
                     `Strong Polarization` = strong_pol)

names(Pop_df) <- c("None", "Small Pol.", "Rare", "Strong Pol.")



Pop_df_plot <- Pop_df %>% pivot_longer(everything(), values_to = "Rating", names_to = "Distr") %>% 
   mutate(Distr = factor(Distr, levels = c("None", "Small Pol.","Rare", "Strong Pol.")))

Pop_df_plot %>% ggplot(aes(Rating, fill = Distr))+
   geom_bar(width= .75)+
   facet_grid(rows = vars(Distr))+
   scale_x_continuous(n.breaks = 11, expand = c(0,0))+
   theme_minimal()+
   theme(strip.text = element_blank(),
         axis.text.y = element_blank(),
         legend.position = "none",
         plot.margin = margin(1, 2, 0, 0),
         axis.text.x = element_text(size = 12))+
   geom_text(aes(x=6,y = 3500, label= Distr, color = Distr), size = 6.8)+
   scale_fill_manual(values = colors)+
   scale_color_manual(values = colors)+
   ylab("Count")
```
  
## Procedure

For this work, we will use different sample sizes consisting of ``r nrow(sampled)`` different sizes:  
```{r Show sample sequence}
sample_sequence
```

For each sample size, we replicate the random draw ``r replications_per_setting`` times. 

As mentioned in the introduction, we will use thresholds for each polarization measure to determine whether polarization is present or not. Luckily, the BC already has a set threshold of $0.\overline{5}$. 

For the two other measures, previous results were consulted. Realization that the measurement of polarization has a low value for the rare distribution (even lower than the normal distribution, as it uses a weighted sum, thus small groups are discounted), leads to a crucial implication. Setting a low threshold for rare distributions in order to identify them as polarized will also net us too many false negatives (e.g. saying normal distributions are polarized). To note, group divergence values in the previous simulation ranged from .24 and .9, while the measure of polarization ranged from .1 to .8. Hence the threshold for group divergence and polarization were (arbitrarily) set at ``r group_divergence_threshold`` and ``r polarization_threshold`` respectively.    

To summarize, 4 different population distribution were created and serve as our effect sizes, the thresholds functions as a proxy for when we actually reject or accept that the sampled distribution is polarized (similar to the alpha value and p-values), and thus we use different effect sizes to estimate the power.

```{r Sampling}
max_cols <- max(sampled)   

# Register parallel backend with the desired number of cores
num_cores <- detectCores()-1

cl <- makeCluster(num_cores)
registerDoParallel(cl)

#rotate Pop_df, so our function works (each row needs to be a different distribution, instead of each col)
rot_Pop_df <- as.data.frame(t(Pop_df))

# Define function to process each combination of risk_distribution, samplesize and replications per setting
sample_and_replicate_for_all_risks <- function(i) {
  sampled_matrix_list <- list()
  
  for (j in 1:nrow(rot_Pop_df)) {
    mat <- replicate(replications_per_setting,
                     sample(1:n_population, size = sampled[i, 1], replace = TRUE)) #create matrix of our samples with replacing, times n - replications
    
    sampled_table <- rot_Pop_df[j, mat] #using the matrix, collect the values from our risk distribution matrix (as a vector though...)
    sampled_matrix <- as.data.frame(matrix(sampled_table,
                                           nrow = replications_per_setting,
                                           ncol = sampled[i, 1],
                                           byrow = TRUE,
                                           dimnames = NULL)) #create df out of these vectors instead of flat vectors
    
    
    if (ncol(sampled_matrix) < max_cols) {
      padding_matrix <- matrix(NA,
                               nrow = nrow(sampled_matrix),
                               ncol = max_cols - ncol(sampled_matrix))
      sampled_matrix <- cbind(sampled_matrix, padding_matrix)
    } #if matrix is not wide enough for our end result matrix, padd it with NA columns, so binding rows is doable (needs same amount of ncols)
    
    sampled_matrix <- cbind(matrix(sampled[i,1], nrow = replications_per_setting), 
                            matrix(j, nrow = replications_per_setting),
                            sampled_matrix) #bind columns with additional information such as sample size and which risk_distribution was sampled
    
    colnames(sampled_matrix) <- c("sample_size", "risk_distribution", paste0("rating_", 1:max_cols)) #rewrite colnames so it is identical to the bigh matrix
    sampled_matrix_list[[j]] <- sampled_matrix #store in list
  }
  return(do.call(rbind, sampled_matrix_list)) #after all risk distributions are sampled from, bind them all and return the output
  
}

# Perform parallel processing using foreach, iterating through the different samplesizes
result <- foreach(i = 1:nrow(sampled), .combine = rbind) %dopar% {
  sample_and_replicate_for_all_risks(i)
}

# Stop the parallel backend
stopCluster(cl)

result <- mutate_all(result, as.numeric )


vis_miss(result[,seq(from =3, to = ncol(result), length.out = 40)], show_perc_col = F)
```  
  
Again, this is how our sampled matrix looks like, adopting a staircase like shape. Using this method saves us time, as well as prevents errors.  

```{r Calculating Polarisation Measures}
#taken from my previous work
calc_group_divergence <- function(vec, midpoint = 6, scale_range = 11){
   
   X_high <- mean(vec[vec >= midpoint], na.rm = TRUE)
   if(sum(vec[vec >= midpoint], na.rm =T ) == 0){
      X_high <- 0
   }
   
   X_low <-  mean(vec[vec < midpoint], na.rm = TRUE)
   if(sum(vec[vec < midpoint], na.rm =T ) == 0){
      X_low <- 0
   }
   return((abs(X_high - X_low)/ scale_range))
}


calc_bimodality_coefficient <- function(vec){
   skew <- skew(vec, na.rm = TRUE, type = 3)
   kurtosis <- kurtosi(vec, na.rm = TRUE, type = 3)
   n <- sum(!is.na(vec))
   return((skew^2+1) / (kurtosis + ((3*((n-1))^2)/((n-2)*(n-3))) ))
}

# Created this one "from scratch", as we work with a smaller scale of 11 instead of 101, the calculation of polarization does not take too long anymore

calc_polarization <- function(vec){
   vec2 <- as.vector(vec)
   freq_vec <- agrmt::collapse(vec2, pos = c(1:11))
   return(agrmt::polarization(freq_vec))
}

# this code is commented, as the computation takes too long, and I hate waiting while kniting. I have saved a sepparate rds file, which will be read in instead. Readers who want to uncomment section, select the lines to uncomment and press Ctrl + Shift + C (on Windows/Linux) or Cmd + Shift + C (on macOS)

# #apply the functions to our result matrix, therefore calculate for each drawn sample the polarization metrics
# BC_result <- apply(result[,-c(1:2)], 1, calc_bimodality_coefficient)
# sum(is.na(BC_result))
# 
# group_divergence_result <- apply(result[,-c(1:2)], 1, calc_group_divergence)
# sum(is.na(group_divergence_result))
# 
# # Register parallel backend with the desired number of cores
# num_cores <- detectCores()-1
# 
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# 
# # Perform parallel processing using foreach, iterating through the different drawn samples
# polarization_result <- foreach(i = 1:nrow(result), .combine = rbind) %dopar% {
#   calc_polarization(result[i, -c(1:2)])
# }
# sum(is.na(polarization_result))
# 
# # Stop the parallel backend
# stopCluster(cl)
# 
# 
# combined_result_measures <- cbind(polarization_result,
#                                   group_divergence_result,
#                                   BC_result,
#                                   result[,1:2]
#                                   )
# sum(is.na(combined_result_measures))
# 
# saveRDS(combined_result_measures, "saved_RDS\\combined_result_measures_power_analysis.rds")

combined_result_measures <- readRDS("saved_RDS\\combined_result_measures_power_analysis.rds")

vis_miss(combined_result_measures, sort_miss = F)
```
  
After we've calculated the polarization measures for each of the ``r nrow(result)`` drawn samples, the measures were put into a data frame. As we can see, no missing values are here, indicating that each measure was calculated successfully.  

## Expectations

It is expected that with greater sample size, we would have a more accurate and reliable estimate of the population distribution. Though if the measures of polarization in the population distributions were lower than our set thresholds, then said measures will never indicate polarization (except due to random noise). To illustrate expected pitfalls of the current approach, here is a fitting analogy, where the polarization measures functions as prediction models in classification problems:  

- Good models with high accuracy can find polarization in case it is polarized (sensitivity), and differentiates distributions as not polarized in case it is not polarized (specificity).  

- If the measure differentiates the distribution with [**no polarization**]{style="color: `r colors[1]`;"} and [**high polarization**]{style="color: `r colors[4]`;"} clearly (e.g. difference of numerical values of said distributions are big), we would correctly categorize them more often according to the threshold.  

- Using the logic from the previous point, if the model itself is accurate, setting the threshold is not as important. For example, if the polarization measure gives two numerical outputs for two different distributions like .2 and .9, then setting the threshold at .5 or .6 would not matter for the categorization itself. In consequence, if the measure is pretty bad at distinguishing the distributions, like .5 and .6, then we have less margin for error regarding the threshold selection, as the thresholds itself needs to be set in between those values to categorize them correctly.  

- Setting the threshold may increase true positives at the expense of false positives and vice verca. One can always set the threshold at .1 or .9, thus having a low or high entry point where one would classify polarization (thus taking a liberal or conservative approach).    

Taking all of these points into account, the power analysis results mainly hinges on the polarization measurements itself and how fitting the threshold were set.  

# Results 

## Overall {.tabset .tabset-pills}

### Polarization Values for the Population Distribution

```{r Display Polarization Measures for all Distributions}
BC_pop <- apply(rot_Pop_df, 1, calc_bimodality_coefficient)
GD_pop <- apply(rot_Pop_df, 1, calc_group_divergence) 
Pol_pop<- apply(rot_Pop_df, 1, calc_polarization)
Pol_measures_pop <- data.frame(BC_pop, GD_pop, Pol_pop)
Pol_measures_pop$`Pol. Distribution` <- rownames(Pol_measures_pop)

Pol_measures_pop %>% 
   select(`Pol. Distribution`, everything()) %>% 
   kable(format = "html", 
         caption = "Polarization Measures in the Population Distributions", 
         digits = 3, 
         col.names = c("Population Distribution", "Bimodality Coefficient", "Group Divergence", "Polarization"), 
         align = "c",
         row.names = FALSE) %>% 
   kableExtra::row_spec(row = 0, angle = -10)
```

### Drawn Sample Table 
```{r Create Dichotome factor of Polarized and None, warning=FALSE, message=FALSE}
#for each measure, use the threshold defined in the methods section
combined_result_measures <- combined_result_measures %>% 
   mutate(BC_Pol = if_else(BC_result >= BC_threshold, 1, 0),
          polarization_Pol = if_else(polarization_result >= polarization_threshold, 1, 0),
          group_divergence_Pol = if_else(group_divergence_result >= group_divergence_threshold, 1, 0),
          transl_risk_distr = factor(risk_distribution, labels = c("None", "Small Pol.", "Rare", "Strong Pol.")))


summarised_result_measures <- combined_result_measures %>% 
   pivot_longer(contains("_Pol"), names_to = "Measure", values_to = "is_polarized") %>% 
   mutate(Measure = str_remove(Measure, "_Pol")) %>%
   group_by(Measure,transl_risk_distr, sample_size) %>% 
   summarise(`Prop_as_polarized_in_%` = sum(is_polarized)/replications_per_setting * 100)%>% 
   ungroup() %>% 
   mutate(Measure = case_when(Measure == "BC"~ "Bimodality Coefficient",
                    Measure == "group_divergence" ~ "Group Divergence",
                    Measure == "polarization" ~ "Polarization"))

summarised_result_measures %>%
   pivot_wider(values_from = `Prop_as_polarized_in_%`,
               names_from = Measure) %>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))
```
### Overall Plot
```{r Plot Results}

summarised_result_measures %>%
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = .9)+
   facet_grid(rows = vars(Measure))+
   scale_x_continuous(breaks = sample_sequence)+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")


```

## Bimodality Coefficient{.tabset .tabset-pills}

### Plot
```{r Individual Plots: Bimodality Coefficient}
summarised_result_measures %>% 
   filter(Measure == "Bimodality Coefficient") %>% 
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = 1.1) +
   scale_x_continuous(breaks = sample_sequence)+
   scale_y_continuous(breaks = seq(0,100, by = 10))+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
      geom_hline(yintercept = c(80,90) , col=c("black","blue"), linetype= "dashed", linewidth = .8)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")+
   annotate(geom = "text", x = 170, y = 82, label= "Power of .8", col = "black")+
   annotate(geom = "text", x = 170, y = 92, label= "Power of .9", col = "blue")

```
  
### Bimodality Coefficient Table
```{r Bimodality Table}
summarised_result_measures %>% 
   filter(Measure == "Bimodality Coefficient")%>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size,
          `Indication towards Polarization (in %)` = `Prop_as_polarized_in_%`) %>% 
   select(-Measure) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))
```
## Group Divergence{.tabset .tabset-pills}

### Plot
```{r Individual Plots: Group Divergence}
summarised_result_measures %>% 
   filter(Measure == "Group Divergence") %>% 
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = 1.1) +
   scale_x_continuous(breaks = sample_sequence)+
   scale_y_continuous(breaks = seq(0,100, by = 10))+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
      geom_hline(yintercept = c(80,90) , col=c("black","blue"), linetype= "dashed", linewidth = .8)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")+
   annotate(geom = "text", x = 170, y = 82, label= "Power of .8", col = "black")+
   annotate(geom = "text", x = 170, y = 92, label= "Power of .9", col = "blue")

```
  
### Group Divergence Table
```{r Group Divergence Table}
summarised_result_measures %>% 
   filter(Measure == "Group Divergence")%>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size,
          `Indication towards Polarization (in %)` = `Prop_as_polarized_in_%`) %>% 
   select(-Measure) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))


```
## Polarization{.tabset .tabset-pills}

### Plot
```{r Individual Plots: Polarization}
summarised_result_measures %>% 
   filter(Measure == "Polarization") %>% 
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = 1.1) +
   scale_x_continuous(breaks = sample_sequence)+
   scale_y_continuous(breaks = seq(0,100, by = 10))+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
      geom_hline(yintercept = c(80,90) , col=c("black","blue"), linetype= "dashed", linewidth = .8)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")+
   annotate(geom = "text", x = 170, y = 82, label= "Power of .8", col = "black")+
   annotate(geom = "text", x = 170, y = 92, label= "Power of .9", col = "blue")

```
  
### Polarization Table
```{r Polarization Table}
summarised_result_measures %>% 
   filter(Measure == "Polarization")%>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size,
          `Indication towards Polarization (in %)` = `Prop_as_polarized_in_%`) %>% 
   select(-Measure) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))
```


# Discussion

Looking over the results, the measures of group divergence and polarization were of no help. Two possible explanations (which is already discussed further above) come to mind:  

- The thresholds were not optimally set (e.g. if the threshold were set in a better position, then we are more likely to find polarization, at the cost of finding false positives as well). For example, the threshold for group divergence was too high, so it never categorized the [**small polarization**]{style="color: `r colors[2]`;"} distribution as polarized. In this case, we've restrained the effect size to be at least at ``r group_divergence_threshold``, which may be too restrictive for this measure.  

- The measure itself have difficulties distinguishing polarization (or at least how I defined polarization). Take for example the [**rare**]{style="color: `r colors[3]`;"} distribution, which I defined as truly polarized, and compare it with the [**not polarized**]{style="color: `r colors[1]`;"} distribution. Their values are near identical. As a reminder, polarization uses weighted sums to calculate polarization. In consequence, it ignores the minority group in the rare distribution, and focuses on the bigger group on the left, which just happens to be a skewed normal distribution.  

On the other hand, the BC delivers a totally acceptable and logical output:  

- All polarized distributions are categorized as polarized, whereas the distribution [**without polarization**]{style="color: `r colors[1]`;"} is categorized as not polarized.  

- With increased sample size, we have more power.  

- The [**strongly polarized distribution**]{style="color: `r colors[4]`;"} reaches a high power level the fastest, wheras [**small**]{style="color: `r colors[2]`;"} and [**rare**]{style="color: `r colors[3]`;"} polarizations are slower due to their implied effect sizes. Bare in mind that rare distributions need bigger sample sizes, so that the probability of sampling from the minority group is sufficiently high.  

But on a more thorough investigation, we see that the simulation for the BC indicates a **sample size of 20 and 30** is enough to get a power of 80% for the [**small polarization distribution**]{style="color: `r colors[2]`;"} and [**rare distribution**]{style="color: `r colors[3]`;"} respectively.... Interpreting this result in a more favorable way, it seems like we've only found the bare minimum sample size needed (and even this interpretation is shaky, as the simulation uses random draws in perfect conditions, which does not translate well to our study). In a more realistic (and thus pessimistic way), the results seem to suggest that this approach (again) does not seem to give a satisfactory answer to our problem at hand. A sample size of 20 and 30 is very small and intuitively leads to unreliable outcomes. Using such a small sample size to infer how a population thinks/ feels about certain risks is out of the question.  

## Limitations

- Only 3 measures of polarization was used, and thus, not every aspect of polarization was covered.  

- Setting the assumption that the [**rare polarization**]{style="color: `r colors[3]`;"} is, in fact, polarized, may not hold well in the study itself, as outliers, misinterpretations or even a mouse slip may contribute to such a distribution, which we would then just assume is polarized.  

- The threshold were set somewhat arbitrary, and therefore, may not hold in the real deal. One might be tempted to change the threshold to find an optimized one where we would have the least false positives and false negatives, but this threshold will only fit the distributions seen here and will not generalize for others. As such, I'll refrain from finding the optimum, as it would not net us any additional insight to our goal.  

- Likewise, one could set the thresholds for the BC even higher, thus restricting the amount of ambivalent distributions which were barely making the cutoff. This would lead to a greater sample size needed, but we would have a more reliable and hopefully a more realistic approximation of how many sampled need to be drawn.  

- As in all classification problems, polarization has no clear cut limit and should be understood as a gradient where some risks are more polarized than others. The threshold implies that distributions just short of the threshold is significantly less polarized then those just above it, which can't be further from the truth.  

- In this simulation, samples were drawn at random. Our sampling procedure may not be as random as in this simulation (maybe only those who answer our questionnaires are those on one pole, among other sampling difficulties).  

- It could also be that the [**"small polarization"**]{style="color: `r colors[2]`;"} is still a big effect size in the real world. If that is the case, using such a small sample will only lead to a lack of power to find polarization.  

## Conclusions

- This approach failed to deliver a clear cut answer in finding the needed sample size to find polarization in case there is one.  

- Only the **lower limit** was found, but due to numerous reasons outlined in the limitations, it is adviced to increase the number...  

- For those who are curious what would happen if we would change the thresholds, feel free to download the code and change the numbers yourself. I've put the variables in the first chunk so it is practical to do so.  

# Credits

## R Packages Used

- agrmt (Ruedin D (2023). _agrmt: Calculate Concentration and Dispersion in Ordered Rating Scales_. R package version 1.42.12,
<https://CRAN.R-project.org/package=agrmt>.)  

- doParallel (Corporation M, Weston S (2022). _doParallel: Foreach Parallel Adaptor for the 'parallel' Package_. R package version 1.0.17,
<https://CRAN.R-project.org/package=doParallel>.)  

- foreach (Microsoft, Weston S (2022). _foreach: Provides Foreach Looping Construct_. R package version 1.5.2, <https://CRAN.R-project.org/package=foreach>.)  

- knitr (Xie Y (2023). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.45, <https://yihui.org/knitr/>.)  

- psych (William Revelle (2023). _psych: Procedures for Psychological, Psychometric, and Personality Research_. Northwestern University, Evanston, Illinois. R
package version 2.3.9, <https://CRAN.R-project.org/package=psych>.)  

- RColorBrewer (Neuwirth E (2022). _RColorBrewer: ColorBrewer Palettes_. R package version 1.1-3, <https://CRAN.R-project.org/package=RColorBrewer>.)  

- rmarkdown (Allaire J, Xie Y, Dervieux C, McPherson J, Luraschi J, Ushey K, Atkins A, Wickham H, Cheng J, Chang W, Iannone R (2023). _rmarkdown: Dynamic
Documents for R_. R package version 2.25, <https://github.com/rstudio/rmarkdown>.)  

- tidyverse (Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM,
Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” _Journal of
Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.)  

- visdat (Tierney N (2017). “visdat: Visualising Whole Data Frames.” _JOSS_, *2*(16), 355. doi:10.21105/joss.00355 <https://doi.org/10.21105/joss.00355>,
<http://dx.doi.org/10.21105/joss.00355>.)  

## Use of AI

ChatGPT 3.5 (OpenAI. 2023, https://chat.openai.com/chat):  

- Symbiotic writing of this whole document. The author wrote the first draft, and used the LLM for rephrasing work, which were mostly taken as inspiration. All sections were equally affected.