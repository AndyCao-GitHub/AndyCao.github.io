---
title: "Risk Polarization Power Analysis"
author: "Andy Cao"
date: "`r Sys.Date()`"
output: 
   html_document:
      #css: styles.css
      toc: true
      number_sections: false
      toc_float: true
      collapsed: true
      smooth_controll: false
      fig.width: 26
      fig.height: 26
      code_download: true
---
```{r Setup, include=FALSE}
library(tidyverse) #data wrangling and other tools for R
library(knitr) # report generation in R
library(psych) #calculate skew and kurtosis for BC
library(agrmt) #for agreement and polarization calculation
library(visdat) #visualize dataframes in plots
library(RColorBrewer) #easy to use color palettes
library(rmarkdown) #for the paged_table function
library(doParallel) #parallel computation using multiple cores
library(foreach) # for each function, so the simulation does not take ages

colors <- brewer.pal(4, "Dark2")

n_scale_on_likert <- c(1:11)
min_likert <- min(n_scale_on_likert)
max_likert <- max(n_scale_on_likert)

sample_sequence <- c(seq(10,200,by =10))
sampled <- data.frame(N_part = sample_sequence)

replications_per_setting <- 200

n_population <- 10^4
prop_minority <- .05

BC_threshold <- 5/9
polarization_threshold <- .5
group_divergence_threshold <- .5

#set seed so every random thing here is reproducible
set.seed(42)

#set echo = FALSE (e.g. dont show code in output) for all chunks, except when explicitly telling otherwise
knitr::opts_chunk$set(
   echo = FALSE,
   warning = TRUE,
   message = TRUE
   )
```
# How Many Ratings Are Needed?

## Introduction

This work builds on my [first (and failed) attempt](Risk-Polarization-Simulation.html) to estimate how many ratings one needs to sample to have a reliable measure of a certain population. Therefore, readers are advised to catch up on my previous work to be up to speed on this smaller project.  

As the unconventional approach did not work, we will revert back to the more "old school" methodologies of an a priori power analysis. That is, estimating the minimum sample size needed to uncover a set effect size (or bigger), given a significance level and type II (false negatives) error.  



# Method

## Population Distributions
As we have no real consensus what polarization entails, getting a numerical quantifier for the effect size seems to be out of the question. 4 preset distributions were chosen again, each with differing grade of polarization, and will serve as proxies for our population distributions. While most of the previous distributions were used again (with the same color as last time) some adjustments were made. Notably, we've transitioned to a 11 -point likert scale, which our group has chosen to use for the upcoming study. Here are the distributions:  

- [**None**]{style="color: `r colors[1]`;"} (a Normal Distribution, but this time with a shifted mean instead of the midpoint), representing a risk where a consensus is established.    
- [**Small polarization**]{style="color: `r colors[2]`;"} (the skewed beta distribution from prior work was replaced with this one, representing a smaller polarization in the population)  
- [**Rare**]{style="color: `r colors[3]`;"} (a majority rating on one extreme, while a minority group with base rate of ``r prop_minority *100``% on the other extreme)  
- [**Strong polarization**]{style="color: `r colors[4]`;"} (still the symmetrical version, but this time in discrete shape)  

It is important to note that while the normal and strong polarization distribution represent no and strong effects, the small and rare distributions should both illustrate small effects in the population, and putting one over the other really depends on how to interpret/ operationalize polarization itself (e.g. asymmetry, distance, agreement etc...).  

```{r Generation of Population Distribution}

normal <- round(rnorm(n_population, mean = 8,sd = 1.2))
normal <- pmin(pmax(normal, min_likert), max_likert)

strong_pol <- round(rbeta(n_population, shape1 = .1, shape2 = .1)*10) +1
strong_pol <- pmin(pmax(strong_pol, min_likert), max_likert)

rare <- c(round(rnorm(n_population *(1- prop_minority), mean = 2,sd = 1.3)), 
           round(rnorm(n_population *prop_minority, mean = 11,sd = .5)))
rare <- pmin(pmax(rare, min_likert), max_likert)

small_pol <- c(round(rnorm(n_population *.65, mean = 5,sd = .9)), 
           round(rnorm(n_population * .35, mean = 10,sd = .5)))
small_pol <- pmin(pmax(small_pol, min_likert), max_likert)


Pop_df <- data.frame(`Normal Distribution`= normal,
                     `Small Polarization` = small_pol,
                     `Rare Polarization` = rare,
                     `Strong Polarization` = strong_pol)

names(Pop_df) <- c("None", "Small Pol.", "Rare", "Strong Pol.")



Pop_df_plot <- Pop_df %>% pivot_longer(everything(), values_to = "Rating", names_to = "Distr") %>% 
   mutate(Distr = factor(Distr, levels = c("None", "Small Pol.","Rare", "Strong Pol.")))

Pop_df_plot %>% ggplot(aes(Rating, fill = Distr))+
   geom_bar(width= .75)+
   facet_grid(rows = vars(Distr))+
   scale_x_continuous(n.breaks = 11, expand = c(0,0))+
   theme_minimal()+
   theme(strip.text = element_blank(),
         axis.text.y = element_blank(),
         legend.position = "none",
         plot.margin = margin(1, 2, 0, 0)
         )+
   geom_text(aes(x=6,y = 3500, label= Distr, color = Distr), size = 6.8)+
   scale_fill_manual(values = colors)+
   scale_color_manual(values = colors)+
   ylab("Count")
```
  
For this work, we will use different sample sizes consisting of ``r nrow(sampled)`` different sizes:  
```{r Show sample sequence}
sample_sequence
```
  
## Procedure
For each sample size, we replicate the random draw ``r replications_per_setting`` times. For each of the samples, the measures of bimodality coefficient (BC), polarization and group divergence are calculated. Similar to classification problems though, we will have to set thresholds for each measure, where values above a threshold indicate polarization, while those falling below indicate the absence of polarization. Luckily, the BC already has a set threshold of $0.\overline{5}$. 

For the two other measures, I looked at the previous results, and the measurement of polarization has a low value for the rare distribution, even lower than the normal distribution (as it uses a weighted sum, thus small groups are discounted). Setting a low threshold so rare distributions are also classified as polarized will therefore also net us too many false negatives (e.g. saying normal distributions are polarized). I'll set the group divergence threshold arbitrarily at ``r group_divergence_threshold`` and polarization threshold at ``r polarization_threshold`` and see how it goes...  

Expectations are that with greater sample size, we would have a more accurate and reliable estimate of the population distribution.

Our polarization measures functions as a sort of prediction models in classification problems. Good models with high accuracy can find polarization in case it is polarized (sensitivity), and differentiates distributions as not polarized in case it is not polarized (specificity). 
If the measure differentiates the distribution with [**no polarization**]{style="color: `r colors[1]`;"} and [**high polarization**]{style="color: `r colors[4]`;"} clearly (e.g. difference of numerical values of said distributions are big), we would correctly categorize them as polarized or not more often according to the threshold. Using this analogy, setting the threshold may increase true positives in trade of with false positives and vice verca.

If the measures of polarization in the population distributions were lower than our set thresholds, then said measure will never indicate polarization (except due to random noise).


```{r Sampling}
max_cols <- max(sampled)   

# Register parallel backend with the desired number of cores
num_cores <- detectCores()-1

cl <- makeCluster(num_cores)
registerDoParallel(cl)

#rotate Pop_df, so our function works (each row needs to be a different distribution, instead of each col)
rot_Pop_df <- as.data.frame(t(Pop_df))

# Define function to process each combination of risk_distribution, samplesize and replications per setting
sample_and_replicate_for_all_risks <- function(i) {
  sampled_matrix_list <- list()
  
  for (j in 1:nrow(rot_Pop_df)) {
    mat <- replicate(replications_per_setting,
                     sample(1:n_population, size = sampled[i, 1], replace = TRUE)) #create matrix of our samples with replacing, times n - replications
    
    sampled_table <- rot_Pop_df[j, mat] #using the matrix, collect the values from our risk distribution matrix (as a vector though...)
    sampled_matrix <- as.data.frame(matrix(sampled_table,
                                           nrow = replications_per_setting,
                                           ncol = sampled[i, 1],
                                           byrow = TRUE,
                                           dimnames = NULL)) #create df out of these vectors instead of flat vectors
    
    
    if (ncol(sampled_matrix) < max_cols) {
      padding_matrix <- matrix(NA,
                               nrow = nrow(sampled_matrix),
                               ncol = max_cols - ncol(sampled_matrix))
      sampled_matrix <- cbind(sampled_matrix, padding_matrix)
    } #if matrix is not wide enough for our end result matrix, padd it with NA columns, so binding rows is doable (needs same amount of ncols)
    
    sampled_matrix <- cbind(matrix(sampled[i,1], nrow = replications_per_setting), 
                            matrix(j, nrow = replications_per_setting),
                            sampled_matrix) #bind columns with additional information such as sample size and which risk_distribution was sampled
    
    colnames(sampled_matrix) <- c("sample_size", "risk_distribution", paste0("rating_", 1:max_cols)) #rewrite colnames so it is identical to the bigh matrix
    sampled_matrix_list[[j]] <- sampled_matrix #store in list
  }
  return(do.call(rbind, sampled_matrix_list)) #after all risk distributions are sampled from, bind them all and return the output
  
}

# Perform parallel processing using foreach, iterating through the different samplesizes
result <- foreach(i = 1:nrow(sampled), .combine = rbind) %dopar% {
  sample_and_replicate_for_all_risks(i)
}

# Stop the parallel backend
stopCluster(cl)

result <- mutate_all(result, as.numeric )


vis_miss(result[,seq(from =3, to = ncol(result), length.out = 40)], show_perc_col = F)
```  
  
Again, this is how our sampled matrix looks like, adopting a staircase like shape. Using this method saves us time, as well as prevents errors.  

```{r Calculating Polarisation Measures}
#taken from my previous work
calc_group_divergence <- function(vec, midpoint = 6, scale_range = 11){
   
   X_high <- mean(vec[vec >= midpoint], na.rm = TRUE)
   if(sum(vec[vec >= midpoint], na.rm =T ) == 0){
      X_high <- 0
   }
   
   X_low <-  mean(vec[vec < midpoint], na.rm = TRUE)
   if(sum(vec[vec < midpoint], na.rm =T ) == 0){
      X_low <- 0
   }
   return((abs(X_high - X_low)/ scale_range))
}


calc_bimodality_coefficient <- function(vec){
   skew <- skew(vec, na.rm = TRUE, type = 3)
   kurtosis <- kurtosi(vec, na.rm = TRUE, type = 3)
   n <- sum(!is.na(vec))
   return((skew^2+1) / (kurtosis + ((3*((n-1))^2)/((n-2)*(n-3))) ))
}

# Created this one "from scratch", as we work with a smaller scale of 11 instead of 101, the calculation of polarization does not take too long anymore

calc_polarization <- function(vec){
   vec2 <- as.vector(vec)
   freq_vec <- agrmt::collapse(vec2, pos = c(1:11))
   return(agrmt::polarization(freq_vec))
}

# this code is commented, as the computation takes too long, and I hate waiting while kniting. I have saved a sepparate rds file, which will be read in instead. Readers who want to uncomment section, select the lines to uncomment and press Ctrl + Shift + C (on Windows/Linux) or Cmd + Shift + C (on macOS)

# #apply the functions to our result matrix, therefore calculate for each drawn sample the polarization metrics
# BC_result <- apply(result[,-c(1:2)], 1, calc_bimodality_coefficient)
# sum(is.na(BC_result))
# 
# group_divergence_result <- apply(result[,-c(1:2)], 1, calc_group_divergence)
# sum(is.na(group_divergence_result))
# 
# # Register parallel backend with the desired number of cores
# num_cores <- detectCores()-1
# 
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# 
# # Perform parallel processing using foreach, iterating through the different drawn samples
# polarization_result <- foreach(i = 1:nrow(result), .combine = rbind) %dopar% {
#   calc_polarization(result[i, -c(1:2)])
# }
# sum(is.na(polarization_result))
# 
# # Stop the parallel backend
# stopCluster(cl)
# 
# 
# combined_result_measures <- cbind(polarization_result,
#                                   group_divergence_result,
#                                   BC_result,
#                                   result[,1:2]
#                                   )
# sum(is.na(combined_result_measures))
# 
# saveRDS(combined_result_measures, "saved_RDS\\combined_result_measures_power_analysis.rds")

combined_result_measures <- readRDS("saved_RDS\\combined_result_measures_power_analysis.rds")

vis_miss(combined_result_measures, sort_miss = F)
```
  
After we've calculated the operationalisation measures for each of the ``r nrow(result)`` drawn samples, the measures were put into a data frame. As we can see, no missing values are here, indicating that each measure was calculated successfully.  

# Results 

## Overall {.tabset .tabset-pills}

### Polarization Values for the Population Distribution

```{r Display Polarization Measures for all Distributions}
BC_pop <- apply(rot_Pop_df, 1, calc_bimodality_coefficient)
GD_pop <- apply(rot_Pop_df, 1, calc_group_divergence) 
Pol_pop<- apply(rot_Pop_df, 1, calc_polarization)
Pol_measures_pop <- data.frame(BC_pop, GD_pop, Pol_pop)
Pol_measures_pop$`Pol. Distribution` <- rownames(Pol_measures_pop)

Pol_measures_pop %>% 
   select(`Pol. Distribution`, everything()) %>% 
   kable(format = "html", 
         caption = "Polarization Measures in the Population Distributions", 
         digits = 3, 
         col.names = c("Population Distribution", "Bimodality Coefficient", "Group Divergence", "Polarization"), 
         align = "c",
         row.names = FALSE) %>% 
   kableExtra::row_spec(row = 0, angle = -10)
```

### Drawn Sample Table 
```{r Create Dichotome factor of Polarized and None, warning=FALSE, message=FALSE}
#for each measure, use the threshold defined in the methods section
combined_result_measures <- combined_result_measures %>% 
   mutate(BC_Pol = if_else(BC_result >= BC_threshold, 1, 0),
          polarization_Pol = if_else(polarization_result >= polarization_threshold, 1, 0),
          group_divergence_Pol = if_else(group_divergence_result >= group_divergence_threshold, 1, 0),
          transl_risk_distr = factor(risk_distribution, labels = c("None", "Small Pol.", "Rare", "Strong Pol.")))


summarised_result_measures <- combined_result_measures %>% 
   pivot_longer(contains("_Pol"), names_to = "Measure", values_to = "is_polarized") %>% 
   mutate(Measure = str_remove(Measure, "_Pol")) %>%
   group_by(Measure,transl_risk_distr, sample_size) %>% 
   summarise(`Prop_as_polarized_in_%` = sum(is_polarized)/replications_per_setting * 100)%>% 
   ungroup() %>% 
   mutate(Measure = case_when(Measure == "BC"~ "Bimodality Coefficient",
                    Measure == "group_divergence" ~ "Group Divergence",
                    Measure == "polarization" ~ "Polarization"))

summarised_result_measures %>%
   pivot_wider(values_from = `Prop_as_polarized_in_%`,
               names_from = Measure) %>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))
```
### Overall Plot
```{r Plot Results}

summarised_result_measures %>%
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = .9)+
   facet_grid(rows = vars(Measure))+
   scale_x_continuous(breaks = sample_sequence)+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")


```

## Bimodality Coefficient{.tabset .tabset-pills}

### Plot
```{r Individual Plots: Bimodality Coefficient}
summarised_result_measures %>% 
   filter(Measure == "Bimodality Coefficient") %>% 
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = 1.1) +
   scale_x_continuous(breaks = sample_sequence)+
   scale_y_continuous(breaks = seq(0,100, by = 10))+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
      geom_hline(yintercept = c(80,90) , col=c("black","blue"), linetype= "dashed", linewidth = .8)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")+
   annotate(geom = "text", x = 170, y = 82, label= "Power of .8", col = "black")+
   annotate(geom = "text", x = 170, y = 92, label= "Power of .9", col = "blue")

```
  
### Bimodality Coefficient Table
```{r Bimodality Table}
summarised_result_measures %>% 
   filter(Measure == "Bimodality Coefficient")%>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size,
          `Indication towards Polarization (in %)` = `Prop_as_polarized_in_%`) %>% 
   select(-Measure) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))
```
## Group Divergence{.tabset .tabset-pills}

### Plot
```{r Individual Plots: Group Divergence}
summarised_result_measures %>% 
   filter(Measure == "Group Divergence") %>% 
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = 1.1) +
   scale_x_continuous(breaks = sample_sequence)+
   scale_y_continuous(breaks = seq(0,100, by = 10))+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
      geom_hline(yintercept = c(80,90) , col=c("black","blue"), linetype= "dashed", linewidth = .8)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")+
   annotate(geom = "text", x = 170, y = 82, label= "Power of .8", col = "black")+
   annotate(geom = "text", x = 170, y = 92, label= "Power of .9", col = "blue")

```
  
### Group Divergence Table
```{r Group Divergence Table}
summarised_result_measures %>% 
   filter(Measure == "Group Divergence")%>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size,
          `Indication towards Polarization (in %)` = `Prop_as_polarized_in_%`) %>% 
   select(-Measure) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))


```
## Polarization{.tabset .tabset-pills}

### Plot
```{r Individual Plots: Polarization}
summarised_result_measures %>% 
   filter(Measure == "Polarization") %>% 
   ggplot(aes(sample_size, `Prop_as_polarized_in_%`, col = transl_risk_distr, group = transl_risk_distr))+
   geom_line(linewidth = 1.1) +
   scale_x_continuous(breaks = sample_sequence)+
   scale_y_continuous(breaks = seq(0,100, by = 10))+
   scale_color_manual(values = colors)+
   theme_minimal()+
   theme(legend.title = element_blank(),
         strip.text = element_blank(),
         legend.position = "top")+
   geom_text(aes(x= sample_sequence[length(sample_sequence)/2+ 3],y = 50, label= Measure), size = 7, inherit.aes = FALSE)+
      geom_hline(yintercept = c(80,90) , col=c("black","blue"), linetype= "dashed", linewidth = .8)+
   ylab("Measure Indicated Polarization (in %)")+
   xlab("Sample Size")+
   annotate(geom = "text", x = 170, y = 82, label= "Power of .8", col = "black")+
   annotate(geom = "text", x = 170, y = 92, label= "Power of .9", col = "blue")

```
  
### Polarization Table
```{r Polarization Table}
summarised_result_measures %>% 
   filter(Measure == "Polarization")%>% 
   rename(`Polarization Grade` = transl_risk_distr, 
          `Sample Size` = sample_size,
          `Indication towards Polarization (in %)` = `Prop_as_polarized_in_%`) %>% 
   select(-Measure) %>% 
   paged_table(options = list(rownames.print = F,
                              rows.print = length(sample_sequence),
                              cols.min.print = 5))
```


# Discussion

As expected using the measures of Group divergence and polarization were of no help. I would ascribe their failures on two things: the threshold were set arbitrarily (e.g. if the threshold were set in a better position, then we are more likely to find polarization, at the cost of finding false positives as well), or the measure itself have difficulties distinguishing polarization (or at least how I defined polarization). Take for example the [**rare**]{style="color: `r colors[3]`;"} distribution, which I defined as truly polarized, but the measure of polarization has a near identical value in comparison for the [**not polarized**]{style="color: `r colors[1]`;"} distribution. Likewise, the threshold for group divergence was too high, so it never categorized the [**small polarization**]{style="color: `r colors[2]`;"} distribution as polarized. In this case, we've restrained the effect size to be at least at ``r group_divergence_result``, 

Though in defense for group divergence, the distribution with small polarization was still set 

The BC 

For those who are curious what would happen if we would change the thresholds, feel free to download the code and change the numbers yourself, I've put them in the first chunk so it is practical to do so.

## Limitations
as like in all classifications, no clear cut line between pol vs none, but gradual.

in accordance to the previous one, setting the assumption that the rare polarization is, in fact, polarized, may not hold well in the study itself, as smaller outliers, misinterpretation or even a mouse slip may contribute to such a distribution, which we would then just assume is polarized.

the treshhold were set somewhat arbitrary, and therefore, may not hold in the real deal, and cannot be generalized for further distributions, but only for the ones seen here. One might change the treshhold to find an optimized one where we would have the least false positives and false negatives, but this threshold will not generalize for other distributions. As such, I'll refrain from finding the optimum, as it would net us not any additional insight to our cause.

likewise, one could set the thresholds for the BC even higher, thus restricting the amount of ambivalent distributions which were barely making the cutoff. This would lead to a greater sample size needed, but we would have a more reliable

only 3 measures of operatioalization was used, and thus, not every aspect of operationalization was covered.

samples were drawn at random. Our sampling procedure may not be as random as in this simulation. Caution is still adviced, or we risk in overestimating our power.

again, take the reesults with a grain of salt again, maybe the "low polarization" is still a big effectsize. If that is the case, using such a small sample will only lead to a lack of power to find polarization.

## Conclusions



# Credits

## Acknowledgements


## R Packages Used

## Use of AI